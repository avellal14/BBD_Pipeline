import tensorflow as tf
from KS_lib.tf_op import layer
from sklearn import metrics
import numpy as np

##############################################################################
def inference(images, keep_prob, flags):

    # reg, green, blue = tf.split(3, 3, images)
    # images = tf.tile(blue, [1, 1, 1, 3])

    #NOTE --> Make sure to understand the dilated convolution, effective field of view, how this works for any sized image
    with tf.variable_scope('context_module'):
        with tf.variable_scope('conv1'):
            conv1 = layer.down_conv_relu_same(images, [3, 3], flags['size_input_patch'][2], 32) # not dilated
        with tf.variable_scope('conv2'):
            conv2 = layer.down_conv_relu_dilated_same(conv1, [3, 3], 32, 32, 1)
        with tf.variable_scope('conv3'):
            conv3 = layer.down_conv_relu_dilated_same(conv2, [3, 3], 32, 32, 2)
        with tf.variable_scope('conv4'):
            conv4 = layer.down_conv_relu_dilated_same(conv3, [3, 3], 32, 32, 4)
        with tf.variable_scope('conv5'):
            conv5 = layer.down_conv_relu_dilated_same(conv4, [3, 3], 32, 32, 8)
        with tf.variable_scope('conv6'):
            conv6 = layer.down_conv_relu_dilated_same(conv5, [3, 3], 32, 32, 16)
        with tf.variable_scope('conv7'):
            conv7 = layer.down_conv_relu_dilated_same(conv6, [3, 3], 32, 32, 32)
        with tf.variable_scope('conv8'):
            conv8 = layer.down_conv_relu_dilated_same(conv7, [3, 3], 32, 32, 64)
        with tf.variable_scope('conv9'):
            conv9 = layer.down_conv_relu_dilated_same(conv8, [3, 3], 32, 32, 1)
        with tf.variable_scope('drop9'):
            drop9 = layer.dropout(conv9, keep_prob) #almost always 50% keep_prob for dropout
        with tf.variable_scope('conv10'):
            conv10 = layer.down_conv_relu_same(drop9, [1, 1], 32, 96) # not dilated
        with tf.variable_scope('drop10'):
            drop10 = layer.dropout(conv10, keep_prob)
        with tf.variable_scope('conv11'):
            conv11 = layer.down_conv_same(drop10, [1, 1], 96, flags['n_classes']) # not dilated
        with tf.variable_scope('softmax'):
            softmax = tf.nn.softmax(conv11)

    return softmax, \
           {'softmax':softmax}

##############################################################################

#calculates the loss based on cross entropy and some other considerations
def loss(softmax, labels, weights, curr_epoch , flags):

    # number of postives and negatives
    epi_mask = tf.to_float(tf.equal(labels,1)) #whether the labels = 1 or not
    stroma_mask = tf.to_float(tf.equal(labels,2)) #whether the labels = 0 or not
    fat_mask = tf.to_float(tf.equal(labels,4))  #whether the labels = 2 or not
    bg_mask = tf.to_float(tf.equal(labels,0)) #whether the label = 3 or not
    intraStr_mask = tf.to_float(tf.equal(labels,3))



    #entries in above boolean tensors are all summed together to get total counts(assuming 4D)
    n_epi = tf.to_float(tf.reduce_sum(epi_mask, [0, 1, 2, 3]))
    print("N_EPI: " + str(tf.rank(epi_mask)))
    n_stroma = tf.to_float(tf.reduce_sum(stroma_mask, [0, 1, 2, 3]))
    n_fat = tf.to_float(tf.reduce_sum(fat_mask, [0, 1, 2, 3]))
    n_bg = tf.to_float(tf.reduce_sum(bg_mask, [0,1,2,3]))
    n_intraStr = tf.to_float(tf.reduce_sum(intraStr_mask, [0,1,2,3]))
    #######################################################
    # class imbalance weight

    max_val = tf.reduce_max(tf.stack([n_epi,n_stroma,n_fat,n_bg]))

    #so the weights are the inverse of the relative frequency so that if 20 +, 30 -, weight(+) = 30/20, weight(-) = 30/30


    epi_weights = tf.cond(tf.greater(n_epi,0.0), lambda: (epi_mask/n_epi) * max_val * 1.4, lambda: epi_mask * max_val * 1.4)
    stroma_weights = tf.cond(tf.greater(n_stroma, 0.0), lambda: (stroma_mask / n_stroma) * max_val * 1.3, lambda: stroma_mask * max_val * 1.3)
    fat_weights = tf.cond(tf.greater(n_fat, 0.0), lambda: (fat_mask / n_fat) * max_val * 1.0, lambda: fat_mask * max_val * 1.0)
    bg_weights = tf.cond(tf.greater(n_bg, 0.0), lambda: (bg_mask / n_bg) * max_val * 1.0, lambda: bg_mask * max_val * 1.0)
    intraStr_weights = tf.cond(tf.greater(n_intraStr, 0.0), lambda: (intraStr_mask / n_intraStr) * max_val * 1.0, lambda: intraStr_mask * max_val * 1.0)


#JIMMYS CODE BELOW
 #   pos_weights = tf.cond(tf.greater(n_pos, 0.0), lambda: (pos_mask / n_pos) * max_val * 1.0,
  #                        lambda: pos_mask * max_val * 1.0)

    #neg_weights = tf.cond(tf.greater(n_neg, 0.0), lambda: (neg_mask / n_neg) * max_val * 1.0,
     #                     lambda: neg_mask * max_val * 1.0)

  #  bd_weights = tf.cond(tf.greater(n_bd, 0.0), lambda: (bd_mask / n_bd) * max_val * 1.0,
   #                      lambda: bd_mask * max_val * 1.0)






    class_weights = epi_weights + stroma_weights + fat_weights + bg_weights + intraStr_weights #start out with [3/2, 0, 0] and [0,1,0], add together --> [3/2,1,0] etc;
    total_weights = class_weights + flags['alpha'] * weights #what is the alpha * weights that is being added to all?

    # sigmoid loss
    labels = tf.squeeze(labels) #removes all extraneous dimensions of size 1
    labels = tf.cast(labels, tf.int64) #makes sure all the labels are integers
    # labels = tf.expand_dims(labels, dim=1)
    onehot = tf.one_hot(labels, depth=flags['n_classes'], on_value=1.0, off_value=0.0, axis=3) #turns the whole labels matrix(vector?) into one-hot
    # onehot = tf.squeeze(onehot, dim=3)

    epsilon = 1e-6
    truncated_softmax = tf.clip_by_value(softmax, epsilon, 1.0 - epsilon) #forces all values to be in range [epsilon, 1-epsilon] to prevent issues with gradients

  #  print("ONEHOT!!!!!!!!: " + str(onehot))
   # print("SOFTMAX!!!!!!!!" + str(truncated_softmax))
    cross_entropy_log_loss = -tf.reduce_sum(onehot * tf.log(truncated_softmax), axis=[3], keep_dims=True)
    cross_entropy_log_loss = (total_weights) * cross_entropy_log_loss #accounts for class imbalance by weighting loss (based on weight matrix)
    avg_cross_entropy_log_loss = tf.reduce_mean(cross_entropy_log_loss, axis=[0, 1, 2]) #finally results in mean cross entropy loss by averaging all entries in all dimensions together

    return {'truncated_softmax': truncated_softmax,
            'cross_entropy_log_loss': cross_entropy_log_loss,
            'avg_cross_entropy_log_loss': avg_cross_entropy_log_loss,
            'labels': labels,
            'onehot': onehot,
            'epi_mask': epi_mask,
            'stroma_mask': stroma_mask,
            'fat_mask':fat_mask,
            'bg_mask':bg_mask,
            'n_epi':n_epi,
            'n_stroma':n_stroma,
            'n_bg':n_bg,
            'n_fat':n_fat,
            'class_weights':class_weights,
            'total_weights':total_weights}

##############################################################################

#uses adam optimizer to train the network
def train(total_loss, global_step, parameters, flags):
    optimizer = tf.train.AdamOptimizer(learning_rate=flags['initial_learning_rate'])
    grads_and_vars = optimizer.compute_gradients(total_loss)
    train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)

    return train_op


##############################################################################

#returns all the useful accuracy statistics including TP, FP, TN, FN, F-1 Score, precision, recall
def accuracy(predicts, labels, flags):
    predicts = tf.reshape(predicts, [-1])
    labels = tf.reshape(labels, [-1])

    TP = [0] * flags['n_classes']
    FP = [0] * flags['n_classes']
    FN = [0] * flags['n_classes']
    TN = [0] * flags['n_classes']
    precision = [0] * flags['n_classes']
    recall = [0] * flags['n_classes']
    f1score = [0] * flags['n_classes']

    for iclass in range(flags['n_classes']):
        TP[iclass] = tf.reduce_sum(tf.to_float(tf.logical_and(tf.equal(predicts, iclass), tf.equal(labels, iclass))))
        FP[iclass] = tf.reduce_sum(tf.to_float(tf.logical_and(tf.equal(predicts, iclass), tf.not_equal(labels, iclass))))
        FN[iclass] = tf.reduce_sum(tf.to_float(tf.logical_and(tf.not_equal(predicts, iclass), tf.equal(labels, iclass))))
        TN[iclass] = tf.reduce_sum(tf.to_float(tf.logical_and(tf.not_equal(predicts, iclass), tf.not_equal(labels, iclass))))

        precision[iclass] = TP[iclass] / tf.to_float(TP[iclass] + FP[iclass])
        recall[iclass] = TP[iclass] / tf.to_float(TP[iclass] + FN[iclass])
        f1score[iclass] = 2 * precision[iclass] * recall[iclass] / tf.to_float(precision[iclass] + recall[iclass])

    return {'TP': TP,
            'FP': FP,
            'FN': FN,
            'TN': TN,
            'precision': precision,
            'recall': recall,
            'f1score': f1score}
