import tensorflow as tf
from KS_lib.tf_op import layer
from sklearn import metrics
import numpy as np

##############################################################################
def inference(images, keep_prob, flags):

    # reg, green, blue = tf.split(3, 3, images)
    # images = tf.tile(blue, [1, 1, 1, 3])
    
    with tf.variable_scope('context_module'):
        with tf.variable_scope('conv1'):
            conv1 = layer.down_conv_relu_same(images, [3, 3], flags['size_input_patch'][2], 32) # not dilated
        with tf.variable_scope('conv2'):
            conv2 = layer.down_conv_relu_dilated_same(conv1, [3, 3], 32, 32, 1)
        with tf.variable_scope('conv3'):
            conv3 = layer.down_conv_relu_dilated_same(conv2, [3, 3], 32, 32, 2)
        with tf.variable_scope('conv4'):
            conv4 = layer.down_conv_relu_dilated_same(conv3, [3, 3], 32, 32, 4)
        with tf.variable_scope('conv5'):
            conv5 = layer.down_conv_relu_dilated_same(conv4, [3, 3], 32, 32, 8)
        with tf.variable_scope('conv6'):
            conv6 = layer.down_conv_relu_dilated_same(conv5, [3, 3], 32, 32, 16)
        with tf.variable_scope('conv7'):
            conv7 = layer.down_conv_relu_dilated_same(conv6, [3, 3], 32, 32, 32)
        with tf.variable_scope('conv8'):
            conv8 = layer.down_conv_relu_dilated_same(conv7, [3, 3], 32, 32, 64)
        with tf.variable_scope('conv9'):
            conv9 = layer.down_conv_relu_dilated_same(conv8, [3, 3], 32, 32, 1)
        with tf.variable_scope('drop9'):
            drop9 = layer.dropout(conv9, keep_prob)
        with tf.variable_scope('conv10'):
            conv10 = layer.down_conv_relu_same(drop9, [1, 1], 32, 48) # not dilated
        with tf.variable_scope('drop10'):
            drop10 = layer.dropout(conv10, keep_prob)
        with tf.variable_scope('conv11'):
            conv11 = layer.down_conv_same(drop10, [1, 1], 48, 3) # not dilated
        with tf.variable_scope('softmax'):
            softmax = tf.nn.softmax(conv11)

    return softmax, \
           {'softmax':softmax}

##############################################################################
def loss(softmax, labels, weights, curr_epoch , flags):

    # number of postives and negatives
    pos_mask = tf.to_float(tf.equal(labels,1))
    neg_mask = tf.to_float(tf.equal(labels,0))
    bd_mask = tf.to_float(tf.equal(labels,2))

    n_pos = tf.to_float(tf.reduce_sum(pos_mask, [0, 1, 2, 3]))
    n_neg = tf.to_float(tf.reduce_sum(neg_mask, [0, 1, 2, 3]))
    n_bd = tf.to_float(tf.reduce_sum(bd_mask, [0, 1, 2, 3]))

    #######################################################
    # class imbalance weight

    max_val = tf.reduce_max(tf.stack([n_pos,n_neg,n_bd]))

    pos_weights = (pos_mask/n_pos)*max_val*1.0
    neg_weights = (neg_mask/n_neg)*max_val*1.0
    bd_weights = (bd_mask/n_bd)*max_val*1.0

    class_weights = pos_weights + neg_weights + bd_weights
    total_weights = class_weights + flags['alpha'] * weights

    # sigmoid loss
    labels = tf.squeeze(labels)
    labels = tf.cast(labels, tf.int64)
    # labels = tf.expand_dims(labels, dim=1)
    onehot = tf.one_hot(labels, depth=flags['n_classes'], on_value=1.0, off_value=0.0, axis=3)
    # onehot = tf.squeeze(onehot, dim=3)

    epsilon = 1e-6
    truncated_softmax = tf.clip_by_value(softmax, epsilon, 1.0 - epsilon)
    cross_entropy_log_loss = -tf.reduce_sum(onehot * tf.log(truncated_softmax), axis=[3], keep_dims=True)
    cross_entropy_log_loss = (total_weights) * cross_entropy_log_loss
    avg_cross_entropy_log_loss = tf.reduce_mean(cross_entropy_log_loss, axis=[0, 1, 2])

    return {'truncated_softmax': truncated_softmax,
            'cross_entropy_log_loss': cross_entropy_log_loss,
            'avg_cross_entropy_log_loss': avg_cross_entropy_log_loss,
            'labels': labels,
            'onehot': onehot,
            'pos_mask': pos_mask,
            'neg_mask': neg_mask,
            'bd_mask':bd_mask,
            'n_pos':n_pos,
            'n_neg':n_neg,
            'n_bd':n_bd,
            'class_weights':class_weights,
            'total_weights':total_weights}

##############################################################################
def train(total_loss, global_step, parameters, flags):
    optimizer = tf.train.AdamOptimizer(learning_rate=flags['initial_learning_rate'])
    grads_and_vars = optimizer.compute_gradients(total_loss)
    train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)

    return train_op


##############################################################################
def accuracy(predicts, labels, flags):
    predicts = tf.reshape(predicts, [-1])
    labels = tf.reshape(labels, [-1])

    TP = [0] * flags['n_classes']
    FP = [0] * flags['n_classes']
    FN = [0] * flags['n_classes']
    TN = [0] * flags['n_classes']
    precision = [0] * flags['n_classes']
    recall = [0] * flags['n_classes']
    f1score = [0] * flags['n_classes']

    for iclass in range(flags['n_classes']):
        TP[iclass] = tf.reduce_sum(tf.to_float(tf.logical_and(tf.equal(predicts, iclass), tf.equal(labels, iclass))))
        FP[iclass] = tf.reduce_sum(tf.to_float(tf.logical_and(tf.equal(predicts, iclass), tf.not_equal(labels, iclass))))
        FN[iclass] = tf.reduce_sum(tf.to_float(tf.logical_and(tf.not_equal(predicts, iclass), tf.equal(labels, iclass))))
        TN[iclass] = tf.reduce_sum(tf.to_float(tf.logical_and(tf.not_equal(predicts, iclass), tf.not_equal(labels, iclass))))

        precision[iclass] = TP[iclass] / tf.to_float(TP[iclass] + FP[iclass])
        recall[iclass] = TP[iclass] / tf.to_float(TP[iclass] + FN[iclass])
        f1score[iclass] = 2 * precision[iclass] * recall[iclass] / tf.to_float(precision[iclass] + recall[iclass])

    return {'TP': TP,
            'FP': FP,
            'FN': FN,
            'TN': TN,
            'precision': precision,
            'recall': recall,
            'f1score': f1score}
